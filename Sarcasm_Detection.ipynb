{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhat132940/CS114.L21.KHCL/blob/main/Sarcasm_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yfc7egt3ARt"
      },
      "source": [
        "## **Thu thập headline từ 6 trang báo**\n",
        "## **Lớp: CS114.L21.KHCL**\n",
        "## **Thành viên nhóm:**\n",
        "- Phan Minh Nhật - 19521956\n",
        "- Lê Võ Tiến Phát - 19521993\n",
        "- Hồ Thịnh - 19522274"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fZ_QiSRCd4W"
      },
      "source": [
        "**Các trang báo châm biếm:**\n",
        "- The Daily Squib - UK: https://www.dailysquib.co.uk/\n",
        "- The Betoota Advocate - Australia : https://www.betootaadvocate.com/\n",
        "- The Onion - US : https://www.theonion.com/\n",
        "\n",
        "**Các trang báo chính thống:**\n",
        "- NBC News - US : https://www.nbcnews.com/\n",
        "- Yahoo News - UK : https://news.yahoo.com/\n",
        "- Daily Telegraph - Australia: https://www.dailytelegraph.com.au/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgnkogrZCWfz"
      },
      "source": [
        "References:\n",
        "- Code này bọn em tham khảo của một nhóm khác. Link: https://github.com/caohungphu/CS114.L21/find/main\n",
        "- https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0nx3tO80Nv"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import json\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ1262tE8GYR"
      },
      "source": [
        "**Code để crawl trang The Onion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0uFlaH8LdfV"
      },
      "source": [
        "def is_leap_year(year):\n",
        "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)    \n",
        "  \n",
        "def getDate(month, year):\n",
        "    day_of_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    if is_leap_year(year):\n",
        "        day_of_month[2] = 29\n",
        "    return day_of_month[month]\n",
        "\n",
        "def getNameMonth(month):\n",
        "    if month < 1 or month > 12:\n",
        "        return 'january'\n",
        "    name_of_month = ['', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
        "    return name_of_month[month]\n",
        "    \n",
        "def getFullDate(day, month, year):\n",
        "    return \"{}/{}/{}\".format(day, month, year)\n",
        "\n",
        "def getArticleFromPage(day, month, year):\n",
        "    urlPage = 'https://www.theonion.com/sitemap/{}/{}/{}'.format(str(year), getNameMonth(month), str(day))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(urlPage, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"h4\", class_=\"sc-1w8kdgf-1 bwRmiu js_sitemap-article\")\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        x = x.find(\"a\")\n",
        "        headline = x.get_text()\n",
        "        article_link = x['href']\n",
        "        is_sarcastic = '1'\n",
        "        result.append([ is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'theonion.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "        \n",
        "def getDataFromPage(day, month, year):\n",
        "    print(\"Date: {}/{}/{}:\".format(day, month, year))\n",
        "    soupArticle = getArticleFromPage(day, month, year)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = 2021\n",
        "    year_stop = 2020\n",
        "    while year > year_stop - 1:\n",
        "        month = 12\n",
        "        if year == 2021:\n",
        "            month = 6\n",
        "        while month:\n",
        "            day = getDate(month, year)\n",
        "            if year == 2021 and month == 6:\n",
        "                day = 7\n",
        "            while day: \n",
        "                getDataFromPage(day, month, year)\n",
        "                day -= 1\n",
        "            month -= 1\n",
        "        year -= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-yjlJt-8RJz"
      },
      "source": [
        "**Code để crawl trang The Betoota Advocate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0KVmJLGAQI6"
      },
      "source": [
        "def getArticleFromPage(page):\n",
        "    API_url = \"https://www.betootaadvocate.com/category/breaking-news/page/{}/\".format(str(page))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"h3\", class_=\"entry-title td-module-title\")\n",
        "    print(response)\n",
        "    print(soupArticle)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        headline = x.find(\"a\")['title']\n",
        "        article_link = x.find(\"a\")['href']\n",
        "        is_sarcastic = '1'\n",
        "        result.append([is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'betootaadvocate.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "        \n",
        "def getDataFromPage(page):\n",
        "    print(\" Page: {}\".format(page)) \n",
        "    soupArticle = getArticleFromPage(page)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    page = 1\n",
        "    while page < 86: #6 January 2018\n",
        "        getDataFromPage(page)\n",
        "        page += 1\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZa4vcDLUebN"
      },
      "source": [
        "**Code để crawl trang The Daily Squib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glmkb3DgSEBT"
      },
      "source": [
        "def getArticleFromPage(page):\n",
        "    API_url = \"https://www.dailysquib.co.uk/category/world/page/{}/\".format(str(page))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"div\", class_=\"td-module-thumb\")\n",
        "    print(response)\n",
        "    print(soupArticle)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        headline = x.find(\"a\")['title']\n",
        "        article_link = x.find(\"a\")['href']\n",
        "        is_sarcastic = '1'\n",
        "        result.append([is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'dailysquib.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "        \n",
        "def getDataFromPage(page):\n",
        "    print(\" Page: {}\".format(page)) \n",
        "    soupArticle = getArticleFromPage(page)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    page = 1\n",
        "    while page < 108: #January 13, 2018\n",
        "        getDataFromPage(page)\n",
        "        page += 1\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9n_iiMIh4e0"
      },
      "source": [
        "**Code để crawl trang NBC News**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQK4pq4QwfNx"
      },
      "source": [
        "import time\n",
        "\n",
        "def getNameMonth(month):\n",
        "    if month < 1 or month > 12:\n",
        "        return 'january'\n",
        "    name_of_month = ['', 'january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n",
        "    return name_of_month[month]\n",
        "\n",
        "def getArticleFromPage(month, year):\n",
        "    API_url = 'https://www.nbcnews.com/archive/articles/{}/{}'.format(str(year), getNameMonth(month))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"main\", class_=\"MonthPage\")\n",
        "    print(response)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        x = x.find(\"a\")\n",
        "        headline = x.get_text()\n",
        "        article_link = x['href']\n",
        "        is_sarcastic = '0'\n",
        "        result.append([ is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'nbcnews.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "\n",
        "def getDataFromPage(month, year):\n",
        "    print(\"{}/{}:\".format(month, year)) \n",
        "    soupArticle = getArticleFromPage(month, year)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = 2021\n",
        "    year_stop = 2018\n",
        "    while year > year_stop - 1:\n",
        "        if year == 2021:\n",
        "            month = 6\n",
        "        else:\n",
        "            month = 12\n",
        "        while month:\n",
        "            getDataFromPage(month, year)\n",
        "            month -= 1\n",
        "        year -= 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDkGl9ncJW7g"
      },
      "source": [
        "**Code để crawl trang Yahoo News**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1S60gmBJWg5"
      },
      "source": [
        "def is_leap_year(year):\n",
        "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)    \n",
        "  \n",
        "def getDate(month, year):\n",
        "    day_of_month = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n",
        "    if is_leap_year(year):\n",
        "        day_of_month[2] = 29\n",
        "    return day_of_month[month]\n",
        "\n",
        "def getFullDate(day, month, year):\n",
        "    return \"{}/{}/{}\".format(day, month, year)\n",
        "\n",
        "def getArticleFromPage(day, month, year):\n",
        "    urlPage = 'https://news.yahoo.com/sitemap/{}_{}_{}'.format(str(year), str(month), str(day))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(urlPage, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"a\", class_=\"Td(n) Td(u):h C($c-fuji-grey-k) Fz(16px) Ta(start) Mb(20px) D(b) Cur(p)\")\n",
        "    print(response)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        headline = x.get_text()\n",
        "        article_link = x['href']\n",
        "        is_sarcastic = '0'\n",
        "        result.append([ is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'yahoo.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "        \n",
        "def getDataFromPage(day, month, year):\n",
        "    print(\"Date: {}_{}_{}:\".format(day, month, year))\n",
        "    soupArticle = getArticleFromPage(day, month, year)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    year = 2021\n",
        "    year_stop = 2018\n",
        "    while year > year_stop - 1:\n",
        "        month = 12\n",
        "        if year == 2021:\n",
        "            month = 6\n",
        "        while month:\n",
        "            day = getDate(month, year)\n",
        "            if year == 2021 and month == 6:\n",
        "                day = 7\n",
        "            while day: \n",
        "                getDataFromPage(day, month, year)\n",
        "                day -= 1\n",
        "            month -= 1\n",
        "        year -= 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mml9yTJGqtG4"
      },
      "source": [
        "**Code để crawl trang Daily Telegraph**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3b_Cx0Hqsph"
      },
      "source": [
        "def getArticleFromPage(page):\n",
        "    API_url = \"https://www.dailytelegraph.com.au/news/breaking-news/page/{}/\".format(str(page))\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'}\n",
        "    response = requests.get(API_url, headers = headers)\n",
        "    soupSite = BeautifulSoup(response.text, 'html.parser')\n",
        "    soupArticle = soupSite.find_all(\"a\", class_=\"storyblock_title_link\")\n",
        "    print(response)\n",
        "    print(soupArticle)\n",
        "    return soupArticle\n",
        "\n",
        "def getHeadlineDataFormArticle(soupArticle):\n",
        "    result = []\n",
        "    for x in soupArticle:\n",
        "        headline = x.get_text()\n",
        "        article_link = x['href']\n",
        "        is_sarcastic = '0'\n",
        "        result.append([is_sarcastic, headline, article_link])\n",
        "    return result\n",
        "\n",
        "def writeFile(headlineData):\n",
        "    fileOutput = 'dailytelegraph.json'\n",
        "    f = open(fileOutput, \"a\", encoding=\"utf-8\")\n",
        "    for i in headlineData:\n",
        "        strOut = \"is_sarcastic:\" + i[0] + \", headline: \" + i[1] + \", article_link: \" + i[2] + \"\\n\"\n",
        "        f.writelines(strOut)\n",
        "    f.close()\n",
        "        \n",
        "def getDataFromPage(page):\n",
        "    print(\" Page: {}\".format(page)) \n",
        "    soupArticle = getArticleFromPage(page)\n",
        "    headlineData = getHeadlineDataFormArticle(soupArticle)\n",
        "    writeFile(headlineData)\n",
        "    print(\"Number of data: \",len(headlineData))\n",
        "    print(\"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    page = 1\n",
        "    while page < 200:\n",
        "        getDataFromPage(page)\n",
        "        page += 1\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26jZrvn6uXx7"
      },
      "source": [
        "## **Báo cáo:**\n",
        "- Bọn em đã thu thập được 64411 headline từ 6 trang báo.\n",
        "- Trong đó các headline crawl được từ The Daily Squib và The Betoota Advocate có các headline bị trùng.\n",
        "- Headline thu thập từ trang Yahoo News có các headline chữ Trung Quốc và các headline bị lỗi (có nhiều dấu ?).\n",
        "Do số lượng headline khá nhiều nên bọn em sẽ xử lí các vấn đề trên sau.\n",
        "Ước tính sau khi xử lí sẽ mất khoảng 6000-10000 headline.\n",
        " "
      ]
    }
  ]
}